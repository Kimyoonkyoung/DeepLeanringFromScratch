# DeepLearning

***

### Data argumentation
입력 이미지를 알고리즘을 동원해 '인워적'으로 확장하는 것
* crop, filp, 밝기변화, 확대축소 등
* 적은 데이터로 학습할 때 매우 효과적임

### 층을 깊게하는 이유
아직까지도 이론적 증거는 부족한 상태
* 신경망의 매개변수가 줄어듬
    * 5*5 = 25, 3*3*2 = 18 로, 더 작은 크기로 깊게하면 매개변수의 갯수가 줄어들 수 있음
    * 작은 필터를 겹쳐서 신경망을 깊게 하면 'receptive field'를 넓힐 수 있음
    * 층을 거듭하면서 ReLU 같은 activation function 을 사이에 끼움으로써 신경망에 '비선형'의 힘을 가하고, 비선형 함수가 겹치면서 더 복잡한 것도 표현이 가능해짐
* 학습의 효율성을 높임
    * 얕은 신경망에서 개를 인식하려면, 특징을 최대한 한번에 이해해야함 (데이터도 풍부하고 다양하게 필요) -> 시간 오래걸림
    * 깊은 신경망에서는 계층적으로 분해가능
        * 초반 층은 에지 학습에 전념하는 식으로, 후반층은 패턴에 전념할 수 있도록
        * 에지를 초반에 추출해 두면, 후반층에서는 그 정보를 활용해서 더 좋은 결과를 낼 수 있음

### CNN model 
top-x오류 : 정답이라고 생각되는 set 중 x순위까지 정답이 없는 오류

#### VGG
합성곱 계층과 풀링 계층으로 구성되는 '기본적인' CNN 구조
* 3*3 작은필터를 사용한 합성곱 계층을 연속으로 거침

#### GoogLeNet
세로 깊이 뿐 아니라, 가로 깊이도 있는 인셉션 구조
* 인셉션 구조
    * 크기가 다른 필터와 풀링을 여러개 적용하여 그 결과를 결합하는 방식
    * 1*1 합성곱은 채널 쪽으로 크기를 줄이는 것
        * 매개변수 제거와 고속처리에 기여함

#### ResNet
층이 매우 깊으면서, 학습이 잘되게한 구조
* skip connection
    * 입력 x를 두개 이상의 합성곱 계층을 건너 뛰어서 출력에 바로 연결하는 방식
    * 역전파때 신호 감쇠를 막아줌 (상류의 기울기를 하류에 그대로 보내기 때문에)
    
### 전이 학습 (transfer learning)
학습된 가중치를 다른 신경망에 복사한다음, 구성이 같은 신경망을 준비하고 미리학습된 가중치로 초깃값을 설정한 후, 새로운 데이터셋을 대상으로 재학습 (fine tuning)을 수행
* 보유한 데이터셋이 적을 때 특히 유용함

### 다양한 기법들

#### GPU 분산
분산학습을 위해 개발중인 toolkit
* 거대한 데이터의 저지연, 고처리량을 위해서 구글의 텐서플로우와 마이크로소프트의 CNTK (Computational Network Toolkit) 가 개발증
* 컴퓨터 사이의 통신과 데이터 동기화 등 

#### R-CNN (Regions with Convolution Neural Network)
사물검출 (detect & recognition) 에 유명한 모델
* 사물의 영역을 찾고 (컴퓨터 비전 분야에서의 다양한 기법), CNN 으로 클래스를 분류

#### 분할 (Segmentation)
이미지를 픽셀수준에서 분류하는 문제
* 픽셀단위로 객체마다 채색된 지도 (supervised) 데이터를 사용해서 학습함
* 각 픽셀 단위로 학습해야해서 시간이 굉장히 오래걸림
* FCN (Fully Convolution Network) 이용 가능
    * 단 한번의 forward 처리로 모든 픽셀의 클래스를 분류해주는 기법
    * 합성곱 계층으로만 이루어진 네트워크
    * 마지막 계층에는 입력 이미지와 같은 크기로 확대(bilinear interpolation)함
    * 확대할 때는 deconvolution 을 수행
    
#### Image Captioning
이미지를 보고 해석할 수 있는 네트워크
* NIC (Neural Image Caption) 이용 가능
    * RNN 이용
        * 순환적으로 관계를 갖는 신경망으로 자연어나 시계열 데이터 등의 연속된 데이터를 다룰 때 많이 활용
        * 과거의 정보를 기억하면서 학습하는 것이 특징
    * CNN에서 특징을 추출하고, RNN에서 텍스트를 순환적으로 생성
    
